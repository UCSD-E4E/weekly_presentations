\begin{frame}{Output of Inference}
    \centering
    \includegraphics[height=0.8\textheight,width=0.8\textwidth,keepaspectratio]{images/mm_inference.png}
\end{frame}

\begin{frame}{Mangrove Watch Resolution}
    \centering
    \includegraphics[height=0.8\textheight,width=0.8\textwidth,keepaspectratio]{images/mm_mangrovewatch.png}
\end{frame}

\begin{frame}{Rough End-to-End Processing Time}
    \begin{itemize}
        \item On 4-core CPU: 20 min (Inference 17 min)
        \item On 3070 GPU: 5 min (Inference 3 min)
    \end{itemize}    
\end{frame}

\begin{frame}{Image Processing Service Docker Image}
    \begin{itemize}
        \item Moved to production-ready pytorch distro
        \item Slimmed image by about 2gb
    \end{itemize}    
\end{frame}

\begin{frame}{Image Processing Service Docker Image cont'd}
    \begin{itemize}
        \item Still is about 12gb
        \item Poetry packages contribute 5gb
    \end{itemize}    
\end{frame}

\begin{frame}{Webserver Refactoring}
    \begin{itemize}
        \item Consistent pings
        \item Fixed bugs with enqueuing system
    \end{itemize}    
\end{frame}
